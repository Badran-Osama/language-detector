/*
 * Copyright 2011 Fabian Kessler
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.optimaize.langdetect;

import com.optimaize.langdetect.ngram.NgramExtractors;
import com.optimaize.langdetect.profiles.LanguageProfile;
import com.optimaize.langdetect.profiles.LanguageProfileReader;
import com.optimaize.langdetect.text.CommonTextObjectFactories;

import org.junit.jupiter.api.BeforeAll;
import org.junit.jupiter.api.DisplayName;
import org.junit.jupiter.params.ParameterizedTest;
import org.junit.jupiter.params.provider.Arguments;
import org.junit.jupiter.params.provider.MethodSource;
import org.testng.annotations.DataProvider;
import org.testng.annotations.Test;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.nio.charset.StandardCharsets;
import java.util.List;
import java.util.stream.Stream;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertFalse;
import static org.junit.Assert.assertThrows;
import static org.junit.Assert.assertTrue;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.testng.Assert.assertThrows;

/**
 * Uses all built-in language profiles and tests some simple clean phrases as well as longer texts  against them
 * with expected outcome.
 *
 * @author Fabian Kessler
 */
public class DataLanguageDetectorImplTest {

	//    private final LanguageDetector shortDetector;
	//    private final LanguageDetector longDetector;
	//
	//    public DataLanguageDetectorImplTest() throws IOException {
	//        List<LanguageProfile> languageProfiles = new LanguageProfileReader().readAllBuiltIn();
	//
	//        shortDetector = LanguageDetectorBuilder.create(NgramExtractors.standard())
	//                .shortTextAlgorithm(100)
	//                .withProfiles(languageProfiles)
	//                .build();
	//
	//        longDetector = LanguageDetectorBuilder.create(NgramExtractors.standard())
	//                .shortTextAlgorithm(0)
	//                .withProfiles(new LanguageProfileReader().readAllBuiltIn())
	//                .build();
	//    }
	//
	//    @Test(dataProvider = "shortCleanTexts")
	//    public void shortTextAlgo(String expectedLanguage, CharSequence text) throws IOException {
	//        assertEquals(shortDetector.getProbabilities(text).get(0).getLocale().getLanguage(), expectedLanguage);
	//        //the detect() method doesn't have enough confidence for all these short texts.
	//    }
	//
	//    @Test(dataProvider = "shortCleanTexts")
	//    public void longTextAlgoWorkingOnShortText(String expectedLanguage, CharSequence text) throws IOException {
	//        assertEquals(longDetector.getProbabilities(text).get(0).getLocale().getLanguage(), expectedLanguage);
	//        //the detect() method doesn't have enough confidence for all these short texts.
	//    }
	//
	//    @Test(dataProvider = "longerWikipediaTexts")
	//    public void longTextAlgoWorkingOnLongText(String expectedLanguage, CharSequence text) throws IOException {
	//        assertEquals(longDetector.getProbabilities(text).get(0).getLocale().getLanguage(), expectedLanguage);
	//        assertEquals(longDetector.detect(text).get().getLanguage(), expectedLanguage);
	//    }
	//
	//    @DataProvider
	//    protected Object[][] shortCleanTexts() {
	//        return new Object[][] {
	//                {"en", shortCleanText("This is some English text.")},
	//                {"fr", shortCleanText("Ceci est un texte franÃ§ais.")},
	//                {"nl", shortCleanText("Dit is een Nederlandse tekst.")},
	//                {"de", shortCleanText("Dies ist eine deutsche Text")},
	//                {"km", shortCleanText("áŸá–áŸ’áœáœá…á“á¶á’á·á”áŸ’á”á¶á™áŸáŸášá¸áŸá˜áŸ’ášá¶á”áŸ‹á¢áŸ’á“á€á‘á¶áŸ†á„á¢áŸáŸ‹á‚áŸ’á“á¶áŸ”" + "á“áŸ…á€áŸ’á“á»á„áœá·á‚á¸á—á¸áŒá¶á—á¶áŸá¶ááŸ’á˜áŸ‚ášá¥á¡á¼áœá“áŸáŸ‡á˜á¶á“ áŸ¡áŸ¡áŸ©áŸ¨ášá¼á”á—á¶á– áŸá˜á¶á‡á·á€áŸ¡áŸ¥áŸ£áŸ£áŸ£á“á¶á€áŸ‹ á“á·á„á˜á¶á“áŸ¤áŸ¥áŸ¨áŸ£á¢ááŸ’áá”á‘áŸ”")},
	//                {"bg", shortCleanText("Ğ•Ğ²Ñ€Ğ¾Ğ¿Ğ° Ğ½Ğµ Ñ‚Ñ€ÑĞ±Ğ²Ğ° Ğ´Ğ° ÑÑ‚Ğ°Ñ€Ñ‚Ğ¸Ñ€Ğ° Ğ½Ğ¾Ğ² ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚ĞµĞ½ Ğ¼Ğ°Ñ€Ğ°Ñ‚Ğ¾Ğ½ Ğ¸ Ğ¸Ğ·Ñ…Ğ¾Ğ´ Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ")},
	//                {"wa", shortCleanText("Ã‡ouchal c' est on tecse pÃ¥r e walon.")},
	//        };
	//    }
	//    private CharSequence shortCleanText(CharSequence text) {
	//        return CommonTextObjectFactories.forDetectingShortCleanText().forText( text );
	//    }
	//
	//    @DataProvider
	//    protected Object[][] longerWikipediaTexts() {
	//        return new Object[][] {
	//                {"de", largeText(readText("/texts/de-wikipedia-Deutschland.txt"))},
	//                {"fr", largeText(readText("/texts/fr-wikipedia-France.txt"))},
	//                {"it", largeText(readText("/texts/it-wikipedia-Italia.txt"))},
	//        };
	//    }
	//
	//    private CharSequence readText(String path) {
	//        try (InputStream inputStream = DataLanguageDetectorImplTest.class.getResourceAsStream(path)) {
	//            try (BufferedReader in = new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8))) {
	//                StringBuilder sb = new StringBuilder();
	//                String str;
	//                while ((str = in.readLine()) != null) {
	//                    sb.append(str);
	//                }
	//                return sb.toString();
	//            }
	//        } catch (IOException e) {
	//            throw new RuntimeException(e);
	//        }
	//    }
	//
	//    private CharSequence largeText(CharSequence text) {
	//        return CommonTextObjectFactories.forDetectingOnLargeText().forText( text );
	//    }


	/**
	 * Added LLM-Generated Test Code using ChatGPT and DeepSeek
	 *
	 * @author Osama Badran
	 */

	//	//Refactored code to have it run Junit 5 from TestNG (this would be Initial Coverage Analysis before adding LLM-Generated Test)
	//    private static LanguageDetector shortDetector;
	//    private static LanguageDetector longDetector;
	//
	//    @BeforeAll
	//    public static void setup() throws IOException {
	//        List<LanguageProfile> languageProfiles = new LanguageProfileReader().readAllBuiltIn();
	//
	//        shortDetector = LanguageDetectorBuilder.create(NgramExtractors.standard())
	//                .shortTextAlgorithm(100)
	//                .withProfiles(languageProfiles)
	//                .build();
	//
	//        longDetector = LanguageDetectorBuilder.create(NgramExtractors.standard())
	//                .shortTextAlgorithm(0)
	//                .withProfiles(languageProfiles)
	//                .build();
	//    }
	//
	//    @ParameterizedTest
	//    @MethodSource("shortCleanTexts")
	//    public void shortTextAlgo(String expectedLanguage, CharSequence text) {
	//        assertEquals(expectedLanguage, shortDetector.getProbabilities(text).get(0).getLocale().getLanguage());
	//    }
	//
	//    @ParameterizedTest
	//    @MethodSource("shortCleanTexts")
	//    public void longTextAlgoWorkingOnShortText(String expectedLanguage, CharSequence text) {
	//        assertEquals(expectedLanguage, longDetector.getProbabilities(text).get(0).getLocale().getLanguage());
	//    }
	//
	//    @ParameterizedTest
	//    @MethodSource("longerWikipediaTexts")
	//    public void longTextAlgoWorkingOnLongText(String expectedLanguage, CharSequence text) {
	//        assertEquals(expectedLanguage, longDetector.getProbabilities(text).get(0).getLocale().getLanguage());
	//        assertEquals(expectedLanguage, longDetector.detect(text).get().getLanguage());
	//    }
	//
	//    private static Stream<Arguments> shortCleanTexts() {
	//        return Stream.of(
	//                Arguments.of("en", shortCleanText("This is some English text.")),
	//                Arguments.of("fr", shortCleanText("Ceci est un texte franÃ§ais.")),
	//                Arguments.of("nl", shortCleanText("Dit is een Nederlandse tekst.")),
	//                Arguments.of("de", shortCleanText("Dies ist eine deutsche Text")),
	//                Arguments.of("km", shortCleanText("áŸá–áŸ’áœáœá…á“á¶á’á·á”áŸ’á”á¶á™áŸáŸášá¸áŸá˜áŸ’ášá¶á”áŸ‹á¢áŸ’á“á€á‘á¶áŸ†á„á¢áŸáŸ‹á‚áŸ’á“á¶áŸ”" +
	//                        "á“áŸ…á€áŸ’á“á»á„áœá·á‚á¸á—á¸áŒá¶á—á¶áŸá¶ááŸ’á˜áŸ‚ášá¥á¡á¼áœá“áŸáŸ‡á˜á¶á“ áŸ¡áŸ¡áŸ©áŸ¨ášá¼á”á—á¶á– áŸá˜á¶á‡á·á€áŸ¡áŸ¥áŸ£áŸ£áŸ£á“á¶á€áŸ‹ á“á·á„á˜á¶á“áŸ¤áŸ¥áŸ¨áŸ£á¢ááŸ’áá”á‘áŸ”")),
	//                Arguments.of("bg", shortCleanText("Ğ•Ğ²Ñ€Ğ¾Ğ¿Ğ° Ğ½Ğµ Ñ‚Ñ€ÑĞ±Ğ²Ğ° Ğ´Ğ° ÑÑ‚Ğ°Ñ€Ñ‚Ğ¸Ñ€Ğ° Ğ½Ğ¾Ğ² ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚ĞµĞ½ Ğ¼Ğ°Ñ€Ğ°Ñ‚Ğ¾Ğ½ Ğ¸ Ğ¸Ğ·Ñ…Ğ¾Ğ´ Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ")),
	//                Arguments.of("wa", shortCleanText("Ã‡ouchal c' est on tecse pÃ¥r e walon."))
	//        );
	//    }
	//
	//    private static CharSequence shortCleanText(CharSequence text) {
	//        return CommonTextObjectFactories.forDetectingShortCleanText().forText(text);
	//    }
	//
	//    private static Stream<Arguments> longerWikipediaTexts() {
	//        return Stream.of(
	//                Arguments.of("de", largeText(readText("/texts/de-wikipedia-Deutschland.txt"))),
	//                Arguments.of("fr", largeText(readText("/texts/fr-wikipedia-France.txt"))),
	//                Arguments.of("it", largeText(readText("/texts/it-wikipedia-Italia.txt")))
	//        );
	//    }
	//
	//    
	//
	//    private static CharSequence readText(String path) {
	//        try (InputStream inputStream = DataLanguageDetectorImplTest.class.getResourceAsStream(path);
	//             BufferedReader in = new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8))) {
	//            StringBuilder sb = new StringBuilder();
	//            String str;
	//            while ((str = in.readLine()) != null) {
	//                sb.append(str);
	//            }
	//            return sb.toString();
	//        } catch (IOException e) {
	//            throw new RuntimeException(e);
	//        }
	//    }
	//    
	//    
	//
	//    private static CharSequence largeText(CharSequence text) {
	//        return CommonTextObjectFactories.forDetectingOnLargeText().forText(text);
	//    }
	//    
	//    //New added test cases for LLM-Generated Test Improvement
	//    
	//    
	//    
	//    
	//    //LLM manual Fix
	//    @ParameterizedTest
	//    @MethodSource("additionalShortCleanTexts")
	//    @DisplayName("Test additional short clean texts for multiple languages")
	//    public void testAdditionalShortCleanTexts(String expectedLanguage, CharSequence text) {
	//    	List<DetectedLanguage> results = shortDetector.getProbabilities(text);
	//    	assertFalse(results.isEmpty(), "No language detected.");
	//
	//    	DetectedLanguage bestMatch = results.get(0);
	//    	assertTrue(bestMatch.getProbability() > 0.85, "Detected language confidence too low.");
	//    	assertEquals(expectedLanguage, bestMatch.getLocale().getLanguage());
	//
	//    }
	//
	//    @ParameterizedTest
	//    @MethodSource("additionalShortCleanTexts")
	//    @DisplayName("Test long text algorithm on additional short clean texts")
	//    public void testLongTextAlgoOnAdditionalShortTexts(String expectedLanguage, CharSequence text) {
	//        assertEquals(expectedLanguage, longDetector.getProbabilities(text).get(0).getLocale().getLanguage());
	//    }
	//
	//    @ParameterizedTest
	//    @MethodSource("additionalLongerWikipediaTexts")
	//    @DisplayName("Test long text algorithm on additional longer texts")
	//    public void testLongTextAlgoOnAdditionalLongTexts(String expectedLanguage, CharSequence text) {
	//        assertEquals(expectedLanguage, longDetector.getProbabilities(text).get(0).getLocale().getLanguage());
	//        assertEquals(expectedLanguage, longDetector.detect(text).get().getLanguage());
	//    }
	//
	//    // Additional test data for multiple languages
	//    private static Stream<Arguments> additionalShortCleanTexts() {
	//        return Stream.of(
	//                Arguments.of("es", shortCleanText("Este es un texto en espaÃ±ol.")),
	//                Arguments.of("it", shortCleanText("Questo Ã¨ un testo in italiano.")),
	//                Arguments.of("ja", shortCleanText("ã“ã‚Œã¯æ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚")),
	//                Arguments.of("ru", shortCleanText("Ğ­Ñ‚Ğ¾ Ñ€ÑƒÑÑĞºĞ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚.")),
	//                Arguments.of("zh", shortCleanText("è¿™æ˜¯ä¸€æ®µä¸­æ–‡æ–‡æœ¬ã€‚")),
	//                Arguments.of("ar", shortCleanText("Ù‡Ø°Ø§ Ù†Øµ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.")),
	//                Arguments.of("tr", shortCleanText("Bu bir TÃ¼rkÃ§e metindir.")),
	//                Arguments.of("pt", shortCleanText("Este Ã© um texto em portuguÃªs."))
	//        );
	//    }
	//
	//    
	//    //LLM manual fix
	//    private static Stream<Arguments> additionalLongerWikipediaTexts() {
	//        return Stream.of(
	//                Arguments.of("es", largeText("EspaÃ±a es un paÃ­s en el sur de Europa conocido por su historia y cultura.")),
	//                Arguments.of("it", largeText("L'Italia Ã¨ famosa per il suo cibo, la sua storia e il suo patrimonio artistico.")),
	//                Arguments.of("ja", largeText("æ—¥æœ¬ã¯æ±ã‚¢ã‚¸ã‚¢ã®å›½ã§ã‚ã‚Šã€ä¼çµ±ã¨æŠ€è¡“é©æ–°ãŒèåˆã—ã¦ã„ã‚‹ã€‚")),
	//                Arguments.of("ru", largeText("Ğ Ğ¾ÑÑĞ¸Ñ - ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ° Ğ² Ğ¼Ğ¸Ñ€Ğµ, Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ² Ğ’Ğ¾ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ•Ğ²Ñ€Ğ¾Ğ¿Ğµ Ğ¸ Ğ¡ĞµĞ²ĞµÑ€Ğ½Ğ¾Ğ¹ ĞĞ·Ğ¸Ğ¸.")),
	//                Arguments.of("zh", largeText("ä¸­å›½æ˜¯ä¸€ä¸ªæ‹¥æœ‰æ‚ ä¹…å†å²å’Œå¤šå…ƒæ–‡åŒ–çš„å›½å®¶ï¼Œä½äºä¸œäºšã€‚")),
	//                Arguments.of("ar", largeText("Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù‡ÙŠ Ù„ØºØ© Ø³Ø§Ù…ÙŠØ© ÙŠØªØ­Ø¯Ø« Ø¨Ù‡Ø§ Ø§Ù„Ù…Ù„Ø§ÙŠÙŠÙ† ÙÙŠ Ø§Ù„Ø´Ø±Ù‚ Ø§Ù„Ø£ÙˆØ³Ø· ÙˆØ´Ù…Ø§Ù„ Ø¥ÙØ±ÙŠÙ‚ÙŠØ§.")),
	//                Arguments.of("tr", largeText("TÃ¼rkiye, Avrupa ve Asya'yÄ± birleÅŸtiren bir Ã¼lkedir ve zengin tarihi ile bilinir.")),
	//                Arguments.of("pt", largeText("Portugal Ã© um paÃ­s europeu conhecido pelos seus navegadores e belas paisagens."))
	//        );
	//    }



	/**
	 * Added Manually Test Improvement
	 *
	 * @author Osama Badran
	 */




	private static LanguageDetector shortDetector;
	private static LanguageDetector longDetector;

	@BeforeAll
	public static void setup() throws IOException {
		List<LanguageProfile> languageProfiles = new LanguageProfileReader().readAllBuiltIn();

		shortDetector = LanguageDetectorBuilder.create(NgramExtractors.standard())
				.shortTextAlgorithm(100)
				.withProfiles(languageProfiles)
				.build();

		longDetector = LanguageDetectorBuilder.create(NgramExtractors.standard())
				.shortTextAlgorithm(0)
				.withProfiles(languageProfiles)
				.build();
	}

	@ParameterizedTest
	@MethodSource("shortCleanTexts")
	public void shortTextAlgo(String expectedLanguage, CharSequence text) {
		assertEquals(expectedLanguage, shortDetector.getProbabilities(text).get(0).getLocale().getLanguage());
	}

	@ParameterizedTest
	@MethodSource("shortCleanTexts")
	public void longTextAlgoWorkingOnShortText(String expectedLanguage, CharSequence text) {
		assertEquals(expectedLanguage, longDetector.getProbabilities(text).get(0).getLocale().getLanguage());
	}

	@ParameterizedTest
	@MethodSource("longerWikipediaTexts")
	public void longTextAlgoWorkingOnLongText(String expectedLanguage, CharSequence text) {
		
		assertEquals(expectedLanguage, longDetector.getProbabilities(text).get(0).getLocale().getLanguage());
		assertEquals(expectedLanguage, longDetector.detect(text).get().getLanguage());
	}

	private static Stream<Arguments> shortCleanTexts() {
		return Stream.of(
				Arguments.of("en", shortCleanText("This is some English text.")),
				Arguments.of("fr", shortCleanText("Ceci est un texte franÃ§ais.")),
				Arguments.of("nl", shortCleanText("Dit is een Nederlandse tekst.")),
				Arguments.of("de", shortCleanText("Dies ist eine deutsche Text")),
				Arguments.of("km", shortCleanText("áŸá–áŸ’áœáœá…á“á¶á’á·á”áŸ’á”á¶á™áŸáŸášá¸áŸá˜áŸ’ášá¶á”áŸ‹á¢áŸ’á“á€á‘á¶áŸ†á„á¢áŸáŸ‹á‚áŸ’á“á¶áŸ”" +
						"á“áŸ…á€áŸ’á“á»á„áœá·á‚á¸á—á¸áŒá¶á—á¶áŸá¶ááŸ’á˜áŸ‚ášá¥á¡á¼áœá“áŸáŸ‡á˜á¶á“ áŸ¡áŸ¡áŸ©áŸ¨ášá¼á”á—á¶á– áŸá˜á¶á‡á·á€áŸ¡áŸ¥áŸ£áŸ£áŸ£á“á¶á€áŸ‹ á“á·á„á˜á¶á“áŸ¤áŸ¥áŸ¨áŸ£á¢ááŸ’áá”á‘áŸ”")),
				Arguments.of("bg", shortCleanText("Ğ•Ğ²Ñ€Ğ¾Ğ¿Ğ° Ğ½Ğµ Ñ‚Ñ€ÑĞ±Ğ²Ğ° Ğ´Ğ° ÑÑ‚Ğ°Ñ€Ñ‚Ğ¸Ñ€Ğ° Ğ½Ğ¾Ğ² ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚ĞµĞ½ Ğ¼Ğ°Ñ€Ğ°Ñ‚Ğ¾Ğ½ Ğ¸ Ğ¸Ğ·Ñ…Ğ¾Ğ´ Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ")),
				Arguments.of("wa", shortCleanText("Ã‡ouchal c' est on tecse pÃ¥r e walon.")),
				//Manually Added 
				Arguments.of("fr", shortCleanText("Bonjour! This is an English-French mix.")),
				Arguments.of("und", shortCleanText("ğ€ğğ‚ğƒğ„ğ…")),  
				Arguments.of("en", shortCleanText("A"))
				);
	}

	private static CharSequence shortCleanText(CharSequence text) {
		return CommonTextObjectFactories.forDetectingShortCleanText().forText(text);
	}

	private static Stream<Arguments> longerWikipediaTexts() {
		return Stream.of(
				Arguments.of("de", largeText(readText("/texts/de-wikipedia-Deutschland.txt"))),
				Arguments.of("fr", largeText(readText("/texts/fr-wikipedia-France.txt"))),
				Arguments.of("it", largeText(readText("/texts/it-wikipedia-Italia.txt"))),


				//Manually Added
				Arguments.of("fr", largeText(readText("/texts/empty-file.txt"))),
				Arguments.of("de", largeText(readText("/texts/missing-file.txt")))


				);
	}



	private static CharSequence readText(String path) {
		try (InputStream inputStream = DataLanguageDetectorImplTest.class.getResourceAsStream(path);
				BufferedReader in = new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8))) {
			StringBuilder sb = new StringBuilder();
			String str;
			while ((str = in.readLine()) != null) {
				sb.append(str);
			}
			return sb.toString();
		} catch (IOException e) {
			throw new RuntimeException(e);
		}
	}



	private static CharSequence largeText(CharSequence text) {
		return CommonTextObjectFactories.forDetectingOnLargeText().forText(text);
	}



}
